{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Vanilla Alexnet Training Script - Normalization Method 2\n",
    "\n",
    "Here the 2D Alexnet model is implemeted using the parameters described in the original Alexnet paper. The data loaded has gone through all the preprocessing steps and is ready to be fed into the CNN without any further steps.\n",
    "\n",
    "This data has been preprocessed using the normalization range of -1100, 600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.keras.layers import Flatten, Conv2D, MaxPooling2D, Dropout, Input, Dense, Activation\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_final = np.load(\"2d_training_data_2.npy\")\n",
    "training_labels = np.load(\"2d_training_labels_2.npy\")\n",
    "testing_images_final = np.load(\"2d_testing_data_2.npy\")\n",
    "testing_labels = np.load(\"2d_testing_labels_2.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callbacks are defined below.\n",
    "\n",
    "Early stopping monitors validation loss and automatically quits the training process if the validation loss has remained constant/has increased for 5 consecutive epochs. \n",
    "\n",
    "Model checkpoint saves the model that performs the best on the validation dataset.\n",
    "\n",
    "Lr_reduction reduces the learning rate by a factor of 0.1 every time the validation loss has remained constant/has increased for 2 consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(filepath='2d_alexnet_vanilla_2.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.1, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is implemented and trained below. Training logs were preserved.\n",
    "\n",
    "In accordance with the original parameters, weight decay is implemented in all the fully connected layers, the learning rate is automatically decreased in training, and a SGD optimization function with a momentum of 0.9 is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9979 samples, validate on 1762 samples\n",
      "Epoch 1/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 134.9634 - acc: 0.6566\n",
      "Epoch 00001: val_loss improved from inf to 116.11316, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 134.7371 - acc: 0.6573 - val_loss: 116.1132 - val_acc: 0.7355\n",
      "Epoch 2/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 99.4496 - acc: 0.8110\n",
      "Epoch 00002: val_loss improved from 116.11316 to 82.92168, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 99.2514 - acc: 0.8109 - val_loss: 82.9217 - val_acc: 0.8207\n",
      "Epoch 3/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 69.0827 - acc: 0.8376\n",
      "Epoch 00003: val_loss improved from 82.92168 to 55.46991, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 68.9185 - acc: 0.8380 - val_loss: 55.4699 - val_acc: 0.8411\n",
      "Epoch 4/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 44.3962 - acc: 0.8430\n",
      "Epoch 00004: val_loss improved from 55.46991 to 33.69253, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 44.2665 - acc: 0.8433 - val_loss: 33.6925 - val_acc: 0.8507\n",
      "Epoch 5/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 25.3566 - acc: 0.8496\n",
      "Epoch 00005: val_loss improved from 33.69253 to 17.55282, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 25.2623 - acc: 0.8501 - val_loss: 17.5528 - val_acc: 0.8627\n",
      "Epoch 6/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 11.9894 - acc: 0.8565\n",
      "Epoch 00006: val_loss improved from 17.55282 to 7.10710, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 11.9306 - acc: 0.8566 - val_loss: 7.1071 - val_acc: 0.8632\n",
      "Epoch 7/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 4.2945 - acc: 0.8570\n",
      "Epoch 00007: val_loss improved from 7.10710 to 2.29293, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 4.2704 - acc: 0.8568 - val_loss: 2.2929 - val_acc: 0.8644\n",
      "Epoch 8/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 1.5170 - acc: 0.8629\n",
      "Epoch 00008: val_loss improved from 2.29293 to 0.90436, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 1.5097 - acc: 0.8627 - val_loss: 0.9044 - val_acc: 0.8632\n",
      "Epoch 9/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.8618\n",
      "Epoch 00009: val_loss improved from 0.90436 to 0.47972, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.6456 - acc: 0.8619 - val_loss: 0.4797 - val_acc: 0.8712\n",
      "Epoch 10/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8802\n",
      "Epoch 00010: val_loss improved from 0.47972 to 0.42640, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 77s 8ms/sample - loss: 0.4510 - acc: 0.8804 - val_loss: 0.4264 - val_acc: 0.8757\n",
      "Epoch 11/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8811\n",
      "Epoch 00011: val_loss improved from 0.42640 to 0.41099, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.4290 - acc: 0.8809 - val_loss: 0.4110 - val_acc: 0.8837\n",
      "Epoch 12/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8959\n",
      "Epoch 00012: val_loss improved from 0.41099 to 0.39473, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.3952 - acc: 0.8962 - val_loss: 0.3947 - val_acc: 0.8927\n",
      "Epoch 13/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.8990\n",
      "Epoch 00013: val_loss improved from 0.39473 to 0.37813, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.3853 - acc: 0.8991 - val_loss: 0.3781 - val_acc: 0.8967\n",
      "Epoch 14/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.9001\n",
      "Epoch 00014: val_loss improved from 0.37813 to 0.36974, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.3748 - acc: 0.9003 - val_loss: 0.3697 - val_acc: 0.9052\n",
      "Epoch 15/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.9043\n",
      "Epoch 00015: val_loss did not improve from 0.36974\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.3639 - acc: 0.9045 - val_loss: 0.3900 - val_acc: 0.8944\n",
      "Epoch 16/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.9117\n",
      "Epoch 00016: val_loss improved from 0.36974 to 0.36779, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.3511 - acc: 0.9116 - val_loss: 0.3678 - val_acc: 0.8978\n",
      "Epoch 17/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.9116\n",
      "Epoch 00017: val_loss improved from 0.36779 to 0.34221, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 78s 8ms/sample - loss: 0.3462 - acc: 0.9121 - val_loss: 0.3422 - val_acc: 0.9103\n",
      "Epoch 18/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.9191\n",
      "Epoch 00018: val_loss did not improve from 0.34221\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.3337 - acc: 0.9187 - val_loss: 0.3728 - val_acc: 0.9018\n",
      "Epoch 19/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.9249\n",
      "Epoch 00019: val_loss did not improve from 0.34221\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.3134 - acc: 0.9250 - val_loss: 0.3621 - val_acc: 0.9047\n",
      "Epoch 20/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9364\n",
      "Epoch 00020: val_loss improved from 0.34221 to 0.27566, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.2754 - acc: 0.9370 - val_loss: 0.2757 - val_acc: 0.9188\n",
      "Epoch 21/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9507\n",
      "Epoch 00021: val_loss improved from 0.27566 to 0.26700, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.2031 - acc: 0.9505 - val_loss: 0.2670 - val_acc: 0.9183\n",
      "Epoch 22/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9538\n",
      "Epoch 00022: val_loss improved from 0.26700 to 0.26135, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1950 - acc: 0.9536 - val_loss: 0.2613 - val_acc: 0.9188\n",
      "Epoch 23/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9534\n",
      "Epoch 00023: val_loss did not improve from 0.26135\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1904 - acc: 0.9536 - val_loss: 0.2678 - val_acc: 0.9149\n",
      "Epoch 24/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9547\n",
      "Epoch 00024: val_loss did not improve from 0.26135\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1830 - acc: 0.9551 - val_loss: 0.2643 - val_acc: 0.9183\n",
      "Epoch 25/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9587\n",
      "Epoch 00025: val_loss improved from 0.26135 to 0.25931, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1760 - acc: 0.9587 - val_loss: 0.2593 - val_acc: 0.9154\n",
      "Epoch 26/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9610\n",
      "Epoch 00026: val_loss improved from 0.25931 to 0.25669, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1680 - acc: 0.9610 - val_loss: 0.2567 - val_acc: 0.9177\n",
      "Epoch 27/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9620\n",
      "Epoch 00027: val_loss improved from 0.25669 to 0.25532, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1671 - acc: 0.9621 - val_loss: 0.2553 - val_acc: 0.9166\n",
      "Epoch 28/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9613\n",
      "Epoch 00028: val_loss did not improve from 0.25532\n",
      "9979/9979 [==============================] - 75s 7ms/sample - loss: 0.1674 - acc: 0.9611 - val_loss: 0.2556 - val_acc: 0.9171\n",
      "Epoch 29/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9621\n",
      "Epoch 00029: val_loss improved from 0.25532 to 0.25458, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1663 - acc: 0.9625 - val_loss: 0.2546 - val_acc: 0.9183\n",
      "Epoch 30/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9624\n",
      "Epoch 00030: val_loss improved from 0.25458 to 0.25423, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1637 - acc: 0.9623 - val_loss: 0.2542 - val_acc: 0.9177\n",
      "Epoch 31/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9629\n",
      "Epoch 00031: val_loss did not improve from 0.25423\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1627 - acc: 0.9629 - val_loss: 0.2547 - val_acc: 0.9200\n",
      "Epoch 32/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9627\n",
      "Epoch 00032: val_loss did not improve from 0.25423\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1627 - acc: 0.9626 - val_loss: 0.2550 - val_acc: 0.9194\n",
      "Epoch 33/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9631\n",
      "Epoch 00033: val_loss did not improve from 0.25423\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1644 - acc: 0.9629 - val_loss: 0.2543 - val_acc: 0.9205\n",
      "Epoch 34/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9626\n",
      "Epoch 00034: val_loss improved from 0.25423 to 0.25418, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1640 - acc: 0.9623 - val_loss: 0.2542 - val_acc: 0.9188\n",
      "Epoch 35/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9628\n",
      "Epoch 00035: val_loss improved from 0.25418 to 0.25410, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1614 - acc: 0.9630 - val_loss: 0.2541 - val_acc: 0.9183\n",
      "Epoch 36/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9638\n",
      "Epoch 00036: val_loss did not improve from 0.25410\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1600 - acc: 0.9638 - val_loss: 0.2542 - val_acc: 0.9183\n",
      "Epoch 37/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9646\n",
      "Epoch 00037: val_loss improved from 0.25410 to 0.25402, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1624 - acc: 0.9649 - val_loss: 0.2540 - val_acc: 0.9183\n",
      "Epoch 38/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9631\n",
      "Epoch 00038: val_loss did not improve from 0.25402\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1603 - acc: 0.9629 - val_loss: 0.2540 - val_acc: 0.9177\n",
      "Epoch 39/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9635\n",
      "Epoch 00039: val_loss improved from 0.25402 to 0.25381, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1638 - acc: 0.9634 - val_loss: 0.2538 - val_acc: 0.9188\n",
      "Epoch 40/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9630\n",
      "Epoch 00040: val_loss improved from 0.25381 to 0.25381, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1617 - acc: 0.9627 - val_loss: 0.2538 - val_acc: 0.9188\n",
      "Epoch 41/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9631\n",
      "Epoch 00041: val_loss did not improve from 0.25381\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1619 - acc: 0.9627 - val_loss: 0.2539 - val_acc: 0.9177\n",
      "Epoch 42/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9625\n",
      "Epoch 00042: val_loss improved from 0.25381 to 0.25376, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1619 - acc: 0.9624 - val_loss: 0.2538 - val_acc: 0.9183\n",
      "Epoch 43/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9623\n",
      "Epoch 00043: val_loss improved from 0.25376 to 0.25374, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 77s 8ms/sample - loss: 0.1614 - acc: 0.9623 - val_loss: 0.2537 - val_acc: 0.9183\n",
      "Epoch 44/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9646\n",
      "Epoch 00044: val_loss improved from 0.25374 to 0.25369, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 77s 8ms/sample - loss: 0.1625 - acc: 0.9650 - val_loss: 0.2537 - val_acc: 0.9183\n",
      "Epoch 45/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9637\n",
      "Epoch 00045: val_loss improved from 0.25369 to 0.25369, saving model to 2d_alexnet_vanilla.h5\n",
      "9979/9979 [==============================] - 77s 8ms/sample - loss: 0.1598 - acc: 0.9639 - val_loss: 0.2537 - val_acc: 0.9183\n",
      "Epoch 46/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9638\n",
      "Epoch 00046: val_loss did not improve from 0.25369\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1617 - acc: 0.9634 - val_loss: 0.2537 - val_acc: 0.9183\n",
      "Epoch 47/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9630\n",
      "Epoch 00047: val_loss did not improve from 0.25369\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1614 - acc: 0.9623 - val_loss: 0.2538 - val_acc: 0.9188\n",
      "Epoch 48/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9645\n",
      "Epoch 00048: val_loss did not improve from 0.25369\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1605 - acc: 0.9646 - val_loss: 0.2537 - val_acc: 0.9183\n",
      "Epoch 49/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9641\n",
      "Epoch 00049: val_loss did not improve from 0.25369\n",
      "9979/9979 [==============================] - 75s 8ms/sample - loss: 0.1613 - acc: 0.9642 - val_loss: 0.2538 - val_acc: 0.9183\n",
      "Epoch 50/50\n",
      "9856/9979 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9652\n",
      "Epoch 00050: val_loss did not improve from 0.25369\n",
      "9979/9979 [==============================] - 76s 8ms/sample - loss: 0.1628 - acc: 0.9653 - val_loss: 0.2537 - val_acc: 0.9183\n",
      "Epoch 00050: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "no_epochs = 50\n",
    "learning_rate = 0.01\n",
    "no_classes = 2\n",
    "validation_split = 0.15\n",
    "verbosity = 1\n",
    "X_train = training_images_final\n",
    "callbacks = [early_stopping, model_checkpoint, lr_reduction]\n",
    "    \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096, input_shape=(227*227*3,), kernel_regularizer=l2(0.0005)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(4096, kernel_regularizer=l2(0.0005)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(no_classes, activation='softmax', kernel_regularizer=l2(0.0005)))\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.SGD(lr=learning_rate, momentum=0.9), metrics=['acc'])\n",
    "\n",
    "history = model.fit(training_images_final, training_labels,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'], label='Accuracy (training data)')\n",
    "plt.plot(history.history['val_acc'], label='Accuracy (validation data)')\n",
    "plt.title('Model performance for 2D CT Scan Cancer Detection')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=(0.5,1.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has now been trained. It is then tested on unseen data in order to obtain the final testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2936/2936 [==============================] - 5s 2ms/sample - loss: 0.2648 - acc: 0.9193\n",
      "Test results - Loss: 0.2647531752852718 - Accuracy: 91.92779064178467%\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(testing_images_final, testing_labels, verbose=1)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, further performance metrics are evaluated using the functions below. These metrics are:\n",
    "\n",
    "Sensitivity: The ability of the model to classify a nodule as malignant when it is actually malignant (TP / (TP+FN))\n",
    "\n",
    "Specificity: The ability of the model to classify a nodule as benign when it is actually benign (TN/ (TN+FP))\n",
    "\n",
    "Precision: The accuracy of the model with regards to identifying malignant cases (i.e., how many of the predicted malignant nodules are actually malignant?) \n",
    "(TP / (TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "# arg1: labels: a list of one-hot encoded binary vectors corresponding to the actual labels of the testing data\n",
    "# arg2: positive: True for positive labels (i.e. malignant), False for negative labels (i.e. benign)\n",
    "\n",
    "# returns: \n",
    "# the number of malignant or benign cases in a one-hot encoded testing labels array\n",
    "\n",
    "def get_true_labels(labels,positive=True):\n",
    "    count = 0\n",
    "    if positive == True:\n",
    "        for x in labels:\n",
    "            if x[1] == 1:\n",
    "                count += 1\n",
    "    else:\n",
    "        for x in labels:\n",
    "            if x[1] == 0:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# parameters:\n",
    "# arg1: actual_labels: a list of one-hot encoded binary vectors corresponding to the actual labels of the testing data\n",
    "# arg2: predicted_labels: a list of integers corresponding to the predictions of the model on the testing data (0=benign, 1=cancer)\n",
    "\n",
    "# returns: \n",
    "# the sensitivity of the model, defined as (TP / (TP+FN))\n",
    "\n",
    "def get_sensitivity(actual_labels, predicted_labels):\n",
    "    total_positives = get_true_labels(actual_labels)\n",
    "    true_positives = 0\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if list(actual_labels[i]) == [0,1] and predicted_labels[i] == 1:\n",
    "            true_positives += 1\n",
    "    return true_positives/total_positives\n",
    "\n",
    "# parameters:\n",
    "# arg1: actual_labels: a list of one-hot encoded binary vectors corresponding to the actual labels of the testing data\n",
    "# arg2: predicted_labels: a list of integers corresponding to the predictions of the model on the testing data (0=benign, 1=cancer)\n",
    "\n",
    "# returns: \n",
    "# the specificity of the model, defined as (TN/ (TN+FP))\n",
    "\n",
    "def get_specificity(actual_labels, predicted_labels):\n",
    "    total_negatives = get_true_labels(actual_labels, False)\n",
    "    true_negatives = 0\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if list(actual_labels[i]) == [1,0] and predicted_labels[i] == 0:\n",
    "            true_negatives += 1\n",
    "    return true_negatives/total_negatives\n",
    "\n",
    "# parameters:\n",
    "# arg1: actual_labels: a list of one-hot encoded binary vectors corresponding to the actual labels of the testing data\n",
    "# arg2: predicted_labels: a list of integers corresponding to the predictions of the model on the testing data (0=benign, 1=cancer)\n",
    "\n",
    "# returns: \n",
    "# the precision of the model, defined as (TP / (TP+FP))\n",
    "\n",
    "def get_precision(actual_labels, predicted_labels):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if list(actual_labels[i]) == [0,1] and predicted_labels[i] == 1:\n",
    "            true_positives += 1\n",
    "        if list(actual_labels[i]) == [1,0] and predicted_labels[i] == 1:\n",
    "            false_positives += 1\n",
    "    return true_positives/(true_positives+false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of model predictions is obtained and, along with the actual labels, is used to derive the performance metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666667 0.8708765315739868 0.928347280334728\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.predict_classes(testing_images_final)\n",
    "\n",
    "sensitivity = get_sensitivity(testing_labels, predicted_labels)\n",
    "specificity = get_specificity(testing_labels, predicted_labels)\n",
    "precision = get_precision(testing_labels, predicted_labels)\n",
    "print(sensitivity, specificity, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the performance metrics are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"2D_AlexNet_Results_Norm2.txt\",\"w+\")\n",
    "f.write(\"2D AlexNet Performance Metrics\\n\")\n",
    "f.write(\"Loss: {0}\\n\".format(test_results[0]))\n",
    "f.write(\"Accuracy: {0}\\n\".format(test_results[1]*100))\n",
    "f.write(\"Sensitivity: {0}\\n\".format(sensitivity))\n",
    "f.write(\"Specificity: {0}\\n\".format(specificity))\n",
    "f.write(\"Precision: {0}\\n\".format(precision))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
